{
 "metadata": {
  "name": "",
  "signature": "sha256:9443d21dfa1c37081f08027722dd0e3a62e0f3b61bf38b35a6577901c54d0d8f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "### 1. IMPORT MODULES\n",
      "###\n",
      "\n",
      "import pandas as pd\n",
      "import time\n",
      "import os\n",
      "import numpy as np\n",
      "import json\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from random import shuffle\n",
      "import pickle\n",
      "import copy\n",
      "\n",
      "def print_elapsed_time():\n",
      "    print \"%0.1f minutes elapsed since 'start' variable reset\" % ((time.time() - start)/60)\n",
      "    \n",
      "def float_proportion(thelist):\n",
      "    counter = 0\n",
      "    try:\n",
      "        for item in thelist:\n",
      "            if item != int(item):\n",
      "                counter += 1\n",
      "        return counter * 1.0 / len(thelist)\n",
      "    except:\n",
      "        return -1\n",
      "    \n",
      "def increment_dict(d, key, increment=1):\n",
      "    if key in d.keys():\n",
      "        d[key] += increment\n",
      "    else:\n",
      "        d[key] = increment\n",
      "    return d\n",
      "\n",
      "parameter_names = ['min',\n",
      "                       'median',\n",
      "                       'max',\n",
      "                       'interquartile',\n",
      "                       'stdev',\n",
      "                       'float_prop',\n",
      "                       'winter_corr',\n",
      "                       'day_corr',\n",
      "                       'jan_median',\n",
      "                       'jul_median',\n",
      "                       'zero_prop',\n",
      "                       'noon_midn_med_dif']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "### 2. SPLIT BIG CSV INTO 10 SMALLER CSVs\n",
      "###\n",
      "\n",
      "# nb without splitting contiguous values in the filenames field\n",
      "\n",
      "do_split = False # since this is a lengthy process that only has to be done once,\n",
      "                 # by default it is switched off\n",
      "\n",
      "if do_split == True:\n",
      "\n",
      "    def split_file(filepath, lines=numlines):\n",
      "    \"\"\"Split a file based on a number of lines, after a change in the first field of a csv. Presumes there is always at least one \n",
      "    field change between each block of lines.\"\"\"\n",
      "    counter = 0\n",
      "    path, filename = os.path.split(filepath)\n",
      "    # filename.split('.') would not work for filenames with more than one .\n",
      "    basename, ext = os.path.splitext(filename)\n",
      "    # open input file\n",
      "    with open(filepath, 'r') as f_in:\n",
      "        try:\n",
      "            # open the first output file\n",
      "            f_out = open(os.path.join(path, '{}_{}{}'.format(basename, counter, ext)), 'w')\n",
      "            status = 'incomplete'\n",
      "            # loop over all lines in the input file, and number them\n",
      "            for i, line in enumerate(f_in):\n",
      "                if i == 0:\n",
      "                    firstline = line\n",
      "                if status == 'waiting for field change':\n",
      "                    if line.strip().split(',')[0] != field:\n",
      "                        f_out.close()\n",
      "                        counter += 1\n",
      "                        print counter\n",
      "                        f_out = open(os.path.join(path, '{}_{}{}'.format(basename, counter, ext)), 'w')\n",
      "                        status = 'incomplete'\n",
      "                        f_out.write(firstline)\n",
      "                if (i > 5 and i % lines == 0):\n",
      "                    status = 'waiting for field change'\n",
      "                    field = line.strip().split(',')[0]\n",
      "                # write the line to the output file\n",
      "                if i <> 1: # remove second line of file, which is not data\n",
      "                    f_out.write(line)\n",
      "        finally:\n",
      "            # close the last output file\n",
      "            f_out.close()\n",
      "\n",
      "    split_file(\"C:/Users/David/Documents/IPython Notebooks/elancematthew/wxdatarawclean.csv\", 567000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### \n",
      "### 3. ADD CALCULATED FIELDS/COLUMNS TO INPUT CSV\n",
      "###\n",
      "\n",
      "# added columns:\n",
      "\n",
      "#     datetime_iso     YYYY-MM-DD-HHMM      Calculated field so data and time are sortable\n",
      "#     winterness       0 to 6               Number of months away from July; maximum is January = 6, Dec. & Feb. = 5, etc.\n",
      "#                                           Used to estimate seasonal trends\n",
      "#     dayness          0 to 12              Number of hours away from midnight; noon = 12, 11am and 1pm = 11, etc.\n",
      "#                                           Used to estimate daily trends\n",
      "\n",
      "do_pickle = False   # since this is a lengthy process that only needs to be done once,\n",
      "                    # by default it is switched off\n",
      "\n",
      "if do_pickle == True:\n",
      "\n",
      "    for filenum in range(10):\n",
      "    \n",
      "        osfilename = 'wxdatarawclean_' + str(filenum) + '.csv'\n",
      "        picklename = 'wxdatamunged_' + str(filenum) + '.pickle'\n",
      "        \n",
      "        print osfilename\n",
      "        \n",
      "        dfin = pd.read_csv(osfilename)\n",
      "        \n",
      "        winterness_conversion = {1:6, 2:5, 12:5, 3:4, 11:4, 4:3, 10:3, 5:2, 9:2, 6:1, 8:1, 7:0}\n",
      "\n",
      "        start = time.time()\n",
      "\n",
      "        def add_iso(df):  \n",
      "            try:\n",
      "                parsed_year = df.Date[-4:]\n",
      "                parsed_month = df.Date[0:2]\n",
      "                parsed_day = df.Date[3:5]\n",
      "                hour = df.Hour\n",
      "                month = int(parsed_month)\n",
      "                dayness = 12 - abs(int(hour/100) - 12)\n",
      "                winterness = winterness_conversion[int(parsed_month)]\n",
      "\n",
      "                # make sure there are no errors in the date and time fields\n",
      "                assert 1930 < int(parsed_year) < 2015\n",
      "                assert 1 <= int(parsed_month) <= 12\n",
      "                assert 1 <= int(parsed_day) <= 31\n",
      "                assert 0 <= hour <= 2400\n",
      "\n",
      "                date_time = parsed_year + '-' + parsed_month + '-' + parsed_day + '-'\n",
      "\n",
      "                if hour < 1000:         #add leading zeroes\n",
      "                    date_time += '0'\n",
      "                if hour < 10 :\n",
      "                    date_time += '0'\n",
      "                if hour == 0:\n",
      "                    date_time += '0'\n",
      "                date_time += str(hour)\n",
      "                return pd.Series({'datetime_iso': date_time, 'winterness':winterness, 'month':month, 'dayness':dayness})\n",
      "            except:\n",
      "                return pd.Series({'datetime_iso': 'ERROR', 'winterness':3, 'month':6, 'dayness':6}) # median values used\n",
      "\n",
      "        dfin = dfin.merge(dfin.apply(add_iso, axis=1), left_index=True, right_index=True)\n",
      "\n",
      "        dfin.to_pickle(picklename)\n",
      "\n",
      "        print_elapsed_time()\n",
      "        print \"If you are reading this in stdout, all dates and times were validated as being in correct range.\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output columns:\n",
      "\n",
      "#     FileName\n",
      "#     datetime_iso     sortable YYYY-MM-DD-HHMM\n",
      "#     battery_voltage  battery voltage (e.g. 1280 = 12.80 V)\n",
      "#     upper_temp       upper elevation temperature (F)\n",
      "#     mid_temp         mid elevation temperature (F)\n",
      "#     lower_temp       lower elevation temperature (F)\n",
      "#     humidity         relative humidity (%)\n",
      "#     wind_min         wind speed minimum past hour (mph)\n",
      "#     wind_avg         wind speed average past hour (mph)\n",
      "#     wind_max         wind speed maximum past hour (mph)\n",
      "#     wind_dir         wind direction vector average past hour (degrees from true north)\n",
      "#     precip           precipitation past hour in hundredths (e.g. 5 = .05 inches)\n",
      "#     snow_24          24 hour snow depth if available (inches)\n",
      "#     snow_total       total snow depth if available (inches)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "### 4. IDENTIFY 'LOW HANGING FRUIT', columns that are most easily identified\n",
      "###\n",
      "\n",
      "do_this_part = True\n",
      "\n",
      "if do_this_part == True:\n",
      "    process_all_pickles = True  # to run the whole script; otherwise it's in debug mode\n",
      "    do_part = 9 #if it is -1, will process entire pickle; ignored if process_all_pickles == True\n",
      "    \n",
      "    ### ignored if process_all_pickles == True:\n",
      "    picklename = \"wxdatamunged_1.pickle\"   # necessary because training set had different header capitalization\n",
      "    # picklename = \"dfin.pickle\" # training set\n",
      "    filename_formatted = 'FileName'\n",
      "    # filename_formatted = 'filename' # training set\n",
      "\n",
      "    \n",
      "    reject_params = {}\n",
      "    machine_params = {}\n",
      "    training_params = {}\n",
      "    \n",
      "    parameters = []\n",
      "    categories = []\n",
      "\n",
      "    samples = []\n",
      "\n",
      "    for pickle_number in range(10):\n",
      "\n",
      "        if process_all_pickles == True or pickle_number == 0:\n",
      "\n",
      "            if process_all_pickles == False:\n",
      "                dfin = pd.read_pickle(picklename)\n",
      "            else:\n",
      "                dfin = pd.read_pickle(\"wxdatamunged_\"+str(pickle_number)+\".pickle\")\n",
      "                print pickle_number\n",
      "\n",
      "            filenames = list(dfin[filename_formatted].unique())\n",
      "\n",
      "            results = {}\n",
      "\n",
      "            if process_all_pickles == True or do_part == -1:\n",
      "                min_filenames = 0\n",
      "                max_filenames = len(filenames)\n",
      "            else:\n",
      "                min_filenames = do_part\n",
      "                max_filenames = do_part+1\n",
      "\n",
      "            for filename_pos in range(min_filenames, max_filenames):\n",
      "                current_filename = filenames[filename_pos]\n",
      "                if process_all_pickles == False and do_part != -1:\n",
      "                    print current_filename\n",
      "                    print \"Note: these are before column ids are possibly changed to bimodal or precipitation_duplicate to undetermined.\"\n",
      "                temp_parameters = []\n",
      "                temp_samples = []\n",
      "                temp_categories = []\n",
      "\n",
      "                dfsub = dfin[dfin[filename_formatted] == current_filename]\n",
      "                dfsub.sort('datetime_iso')\n",
      "\n",
      "                if picklename == \"dfin.pickle\": #training set\n",
      "                    columns_to_id = ['raw06', 'raw07', 'raw08', 'raw09', 'raw10', 'raw11', 'raw12', 'raw13'] #, 'raw14', 'raw15', 'raw16']\n",
      "                else:\n",
      "                    columns_to_id = ['raw06', 'raw07', 'raw08', 'raw09', 'raw10', 'raw11', 'raw12', 'raw13',\n",
      "                                     'raw14', 'raw15', 'raw16']\n",
      "\n",
      "                columns_to_id = list(reversed(columns_to_id))\n",
      "\n",
      "                # note that these counters were used to build the example table for approval\n",
      "                # after that I considered them no longer useful as column header suffixes\n",
      "                # but some are still used as initial categorization criteria\n",
      "                # therefore due to time constraints instead of recoding,\n",
      "                # they are all assigned as column header suffixes and then removed later\n",
      "                \n",
      "                undeterminable_counter = 0\n",
      "                undetermined_counter = 0\n",
      "                temperature_counter = 0\n",
      "                precipitation_counter = 0\n",
      "                decimal_precipitation_counter = 0\n",
      "                wind_dir_counter = 0\n",
      "                bimodal_counter = 0\n",
      "                cols_to_skip = []\n",
      "\n",
      "                column_ids = {}\n",
      "                column_position = 0\n",
      "                for column_position in range(len(columns_to_id)):\n",
      "                    cur_col = columns_to_id[column_position]\n",
      "                    colall = dfsub[cur_col]\n",
      "                    colpos = dfsub[dfsub[cur_col] > -49][cur_col]\n",
      "\n",
      "                    #### note, only values between 15th and 85th percentile are used, to eliminate outliers ####\n",
      "                    \n",
      "                    zero_prop = len(dfsub[dfsub[cur_col] == 0.0])/(1.0*(len(dfsub)+1))\n",
      "\n",
      "                    #trim to remove outliers\n",
      "                    remove_low = False\n",
      "                    remove_high = False\n",
      "                    if colpos.quantile(.15) > colpos.min():\n",
      "                        remove_low = True\n",
      "                    if colpos.quantile(.85) < colpos.max():\n",
      "                        remove_high = False\n",
      "                    if remove_high == True and remove_low == True:\n",
      "                        colpos = colpos[(colpos>colpos.quantile(.15)) & (colpos<colpos.quantile(.85))]\n",
      "                    elif remove_high == True:\n",
      "                        colpos = colpos[colpos<colpos.quantile(.85)]\n",
      "                    elif remove_low == True:\n",
      "                        colpos = colpos[colpos>colpos.quantile(.15)]\n",
      "\n",
      "                    #used only for windspeed\n",
      "                    #note that other columns are only used in this step,\n",
      "                    #not in machine learning, to avoid overfitting\n",
      "                    if column_position+3 < len(columns_to_id):\n",
      "                        colpos2 = dfsub[dfsub[columns_to_id[column_position+1]] > -49][columns_to_id[column_position+1]]\n",
      "\n",
      "                    float_prop = float_proportion(list(colpos))\n",
      "                    if 0.3 < float_prop < 0.7:\n",
      "                        bimodal_counter += 1\n",
      "\n",
      "                    if column_position not in cols_to_skip:\n",
      "                        sdev = colpos.std()\n",
      "\n",
      "                        # determine whether there are enough days and months in the sample to use this metric\n",
      "                        use_winterness, use_dayness = False, False\n",
      "                        if (dfsub[dfsub.winterness > -99]['winterness'].min() <= 1 and dfsub[dfsub.winterness > -99]['winterness'].max() >= 5):\n",
      "                            use_winterness = True\n",
      "                            winter_correlation = colpos.corr(dfsub[dfsub.winterness > -99]['winterness'], method='spearman')\n",
      "                        else:\n",
      "                            winter_correlation = -99\n",
      "                        if (dfsub[dfsub.dayness > -99]['dayness'].min() <= 1 and dfsub[dfsub.dayness > -99]['dayness'].max() >= 11):\n",
      "                            use_dayness = True\n",
      "                            day_correlation = colpos.corr(dfsub[dfsub.dayness > -99]['dayness'], method='spearman')\n",
      "                        else:\n",
      "                            day_correlation = -99\n",
      "\n",
      "                        # for snowfall\n",
      "                        janmedian = dfsub[(dfsub[cur_col] > -49) & (dfsub.winterness == 6)][cur_col].median()\n",
      "                        julmedian = dfsub[(dfsub[cur_col] > -49) & (dfsub.winterness == 0)][cur_col].median()\n",
      "\n",
      "                        #for machine learning\n",
      "                        noon_midn_med_dif = (dfsub[(dfsub[cur_col] > -49) & (dfsub.dayness == 12)][cur_col].median()- \n",
      "                                            dfsub[(dfsub[cur_col] > -49) & (dfsub.dayness == 0)][cur_col].median())\n",
      "\n",
      "                        # if all values are -49 or less:\n",
      "                        if len(colpos) == 0:\n",
      "                            undeterminable_counter += 1\n",
      "                            column_ids[cur_col] = 'undeterminable_' + str( undeterminable_counter )\n",
      "\n",
      "                        # if more than 98 percent of values are -49 or less\n",
      "                        elif colall.quantile(.98) <= -49:\n",
      "                            undeterminable_counter += 1\n",
      "                            column_ids[cur_col] = 'undeterminable_' + str(undeterminable_counter)\n",
      "\n",
      "                        #evaluate whether it is battery voltage\n",
      "                        # 20th and 80th percentile are between 5 and 15, 10 and 16\n",
      "                        # standard devation < 0.1\n",
      "                        # float proportion greater than .5\n",
      "                        elif (5 <= colpos.min() < 15 and\n",
      "                              10 <= colpos.max() < 16 and\n",
      "                              sdev < 1 and\n",
      "                              float_prop > 0.5):\n",
      "                            column_ids[cur_col] = \"battery_voltage\"\n",
      "\n",
      "                        # evaluate whether it is wind direction\n",
      "                        # DEPRECATED criteria: min 0-10, max 350-360, median is within 15 degrees of average of 25 percentile and 75 percentile\n",
      "                        #OR maximum between 359 and 360\n",
      "                        # \n",
      "                        elif (column_position < len(columns_to_id) - 3 and\n",
      "                              sdev > 10 and day_correlation < 0.1 and 90 < colpos.median() < 270 and\n",
      "                              colpos.max() < 350):\n",
      "                            wind_dir_counter += 1\n",
      "                            if wind_dir_counter == 1:\n",
      "                                column_ids[cur_col] = 'wind_dir'\n",
      "                                if colpos2.std() > 0:\n",
      "                                    column_ids[columns_to_id[column_position+1]] = 'wind_max'\n",
      "                                    column_ids[columns_to_id[column_position+2]] = 'wind_avg'\n",
      "                                    column_ids[columns_to_id[column_position+3]] = 'wind_min'\n",
      "                                else: #column before wind_dir is nan; either following two columns or following\n",
      "                                     #three columns are wind speeds\n",
      "                                    if column_position < len(columns_to_id) - 4:\n",
      "                                        switch = True\n",
      "                                    else:\n",
      "                                        switch = False\n",
      "                                    if switch == True:\n",
      "                                        colpos3 = dfsub[dfsub[columns_to_id[column_position+2]] > -49][columns_to_id[column_position+2]]        \n",
      "                                        colpos4 = dfsub[dfsub[columns_to_id[column_position+3]] > -49][columns_to_id[column_position+3]]    \n",
      "                                        colpos5 = dfsub[dfsub[columns_to_id[column_position+4]] > -49][columns_to_id[column_position+4]]\n",
      "                                        if not (  0 <= (colpos5.quantile(.25) - colpos4.quantile(.25)) < 30 and\n",
      "                                              0 <= (colpos4.quantile(.25) - colpos3.quantile(.25)) < 30 and\n",
      "                                              0 < (colpos5.quantile(.5) - colpos4.quantile(.5)) < 30 and\n",
      "                                              0 < (colpos4.quantile(.5) - colpos3.quantile(.5)) < 30 and\n",
      "                                              0 < (colpos5.quantile(.75) - colpos4.quantile(.75)) < 30 and\n",
      "                                              0 < (colpos4.quantile(.75) - colpos3.quantile(.75)) < 30):\n",
      "                                            switch = False\n",
      "                                    if switch == True:\n",
      "                                        cols_to_skip.append(column_position+4)  #other three are below\n",
      "                                        column_ids[columns_to_id[column_position+2]] = 'wind_max'\n",
      "                                        column_ids[columns_to_id[column_position+3]] = 'wind_avg'\n",
      "                                        column_ids[columns_to_id[column_position+4]] = 'wind_min'\n",
      "                                    else: # only two following columns are windspeed    \n",
      "                                        column_ids[columns_to_id[column_position+2]] = 'wind_max'\n",
      "                                        column_ids[columns_to_id[column_position+3]] = 'wind_min'\n",
      "                                    if columns_to_id[column_position+1] not in column_ids.keys(): #might have already been identified as bad data above\n",
      "                                        undetermined_counter += 1\n",
      "                                        column_ids[columns_to_id[column_position+1]] = 'undetermined_' + str(undetermined_counter)\n",
      "\n",
      "                                cols_to_skip.append(column_position+1)\n",
      "                                cols_to_skip.append(column_position+2)\n",
      "                                cols_to_skip.append(column_position+3)\n",
      "                            else:\n",
      "                                column_ids[cur_col] = 'wind_dir_duplicate_' + str(wind_dir_counter)    \n",
      "                                \n",
      "                                \n",
      "                        # evaluate whether it is snowfall\n",
      "\n",
      "                        elif (janmedian > 30 and (np.isnan(julmedian) or julmedian == 0)):\n",
      "                            column_ids[cur_col] = 'snow_total'\n",
      "                        elif (janmedian > 1 and (np.isnan(julmedian) or julmedian == 0)):\n",
      "                            column_ids[cur_col] = 'snow_24'\n",
      "                            \n",
      "                        # evaluate whether it is precipitation\n",
      "                        elif zero_prop > 0.3:\n",
      "                            precipitation_counter += 1\n",
      "                            if 0 <= colpos.median() < 1:\n",
      "                                decimal_precipitation_counter += 1\n",
      "                                column_ids[cur_col] = 'precipitation_' + str(precipitation_counter)\n",
      "                            else:\n",
      "                                column_ids[cur_col] = 'precipitation_duplicate_' + str(precipitation_counter)\n",
      "                            ### note the above code was 'hacked' from existing code; here's what happens:\n",
      "                            # if columns is as yet uncategorized and if more than 30% of values = 0\n",
      "                            # if median < 1, marked \"precipitation_1\", \"precipitation_2\", etc. (never just \"precipitation\")\n",
      "                            # otherwise (median >-1), marked \"precipitation_duplicate_1\", etc.\n",
      "                            # so they will contain suffix numbers and the word 'duplicate' even if they are not a duplicate\n",
      "                            # this doesn't matter, as these get routinely sliced off later.\n",
      "                            # it just serves to identify \"definite\" precipitation with less definite ones.\n",
      "                            # it's inelegant, but it's a quick fit\n",
      "\n",
      "                        # evaluate whether it is temperature\n",
      "                        elif (use_winterness == True and\n",
      "                              use_dayness == True and\n",
      "                              colpos.max() < 100 and\n",
      "                              0 < colpos.median() < 70 and\n",
      "                              -0.9 <= winter_correlation <= 0.05 and\n",
      "                              -0.1 <= day_correlation <= 0.5 and sdev > 0.1):\n",
      "                            temperature_counter += 1\n",
      "                            column_ids[cur_col] = 'temperature_' + str(temperature_counter)\n",
      "\n",
      "                        # evaluate whether it is humidity\n",
      "                        elif ((97 <= colpos.max() <= 100 or (colpos.quantile(.25) > 30 and\n",
      "                            colpos.max() <= 100)) and\n",
      "                            -0.2 < winter_correlation < 0.75 and\n",
      "                            -0.8 < day_correlation < 0.1):\n",
      "                            column_ids[cur_col] = 'humidity'\n",
      "\n",
      "\n",
      "\n",
      "                        elif use_winterness == False or use_dayness == False:\n",
      "                            undeterminable_counter += 1\n",
      "                            column_ids[cur_col] = 'undeterminable_' + str( undeterminable_counter)\n",
      "\n",
      "                        else:\n",
      "                            undetermined_counter += 1\n",
      "                            column_ids[cur_col] = 'undetermined_' + str( undetermined_counter)\n",
      "\n",
      "                    if column_ids[cur_col][:8] != 'undeterm':\n",
      "                        if np.isnan(julmedian):\n",
      "                            julmedian= -47\n",
      "                        category_col = column_ids[cur_col]\n",
      "                        category_col = category_col.replace('_24', '_xxx')\n",
      "                        for i in range(1,9):\n",
      "                            category_col = category_col.replace('_'+str(i), '')\n",
      "                            category_col = category_col.replace('_duplicate', '')\n",
      "                        category_col = category_col.replace('_xxx', '_24')\n",
      "                        key = current_filename+\"__\"+cur_col\n",
      "                        temp_samples.append(key)\n",
      "                        temp_categories.append(category_col)\n",
      "                        templist = []\n",
      "                        templist.append([colpos.min(),\n",
      "                                        colpos.median(),\n",
      "                                        colpos.max(),\n",
      "                                        colpos.quantile(.75) - colpos.quantile(.25),\n",
      "                                        sdev,\n",
      "                                        float_prop,\n",
      "                                        winter_correlation,\n",
      "                                        day_correlation,\n",
      "                                        janmedian,\n",
      "                                        julmedian,\n",
      "                                        zero_prop,\n",
      "                                        noon_midn_med_dif])\n",
      "                        temp_parameters.append(templist)\n",
      "                        training_params[key] = templist\n",
      "                        \n",
      "                    if column_ids[cur_col][:14] == 'undeterminable':\n",
      "                        if np.isnan(julmedian):\n",
      "                            julmedian= -47\n",
      "                        key = current_filename+\"__\"+cur_col\n",
      "                        templist = []\n",
      "                        templist.append([colpos.min(),\n",
      "                                        colpos.median(),\n",
      "                                        colpos.max(),\n",
      "                                        colpos.quantile(.75) - colpos.quantile(.25),\n",
      "                                        sdev,\n",
      "                                        float_prop,\n",
      "                                        winter_correlation,\n",
      "                                        day_correlation,\n",
      "                                        janmedian,\n",
      "                                        julmedian,\n",
      "                                        zero_prop,\n",
      "                                        noon_midn_med_dif])\n",
      "                        reject_params[key] = templist\n",
      "                        \n",
      "                    if column_ids[cur_col][:12] == 'undetermined':\n",
      "                        if np.isnan(julmedian):\n",
      "                            julmedian= -47\n",
      "                        key = current_filename+\"__\"+cur_col\n",
      "                        templist = []\n",
      "                        templist.append([colpos.min(),\n",
      "                                        colpos.median(),\n",
      "                                        colpos.max(),\n",
      "                                        colpos.quantile(.75) - colpos.quantile(.25),\n",
      "                                        sdev,\n",
      "                                        float_prop,\n",
      "                                        winter_correlation,\n",
      "                                        day_correlation,\n",
      "                                        janmedian,\n",
      "                                        julmedian,\n",
      "                                        zero_prop,\n",
      "                                        noon_midn_med_dif])\n",
      "                        machine_params[key] = templist\n",
      "\n",
      "                    if len(column_ids) > len(columns_to_id):\n",
      "                            print \"Column(s) missing\"\n",
      "                            print column_ids\n",
      "                \n",
      "                    if process_all_pickles == False and do_part != -1: #review one filename\n",
      "                        print cur_col.ljust(6),\n",
      "                        print (\"%0.1f\" % (colpos.min())).rjust(5),\n",
      "                        print (\"%0.1f\" % (colpos.quantile(.25))).rjust(5),\n",
      "                        print (\"%0.1f\" % (colpos.quantile(.50))).rjust(5),\n",
      "                        print (\"%0.1f\" % (colpos.quantile(.75))).rjust(5),\n",
      "                        print (\"%0.1f\" % (colpos.max())).rjust(5),\n",
      "                        print (\"%0.1f\" % (colpos.mean())).rjust(5),\n",
      "                        print (\"%0.3f\" % (winter_correlation)).rjust(7),\n",
      "                        print (\"%0.3f\" % (day_correlation)).rjust(7),\n",
      "                        print (\"%0.3f\" % (zero_prop)).rjust(7),\n",
      "                        print (\"%0.3f\" % (sdev)).rjust(7),\n",
      "                        print (\"%0.3f\" % (float_prop)),\n",
      "                        #print ' '\n",
      "                        print column_ids[cur_col]\n",
      "                \n",
      "                if decimal_precipitation_counter > 0:\n",
      "                    for key in column_ids.keys():\n",
      "                        if column_ids[key].find('precipitation_duplicate') != -1:\n",
      "                            column_ids[key] = 'undetermined' # no suffix number given or needed because they ended up not being used\n",
      "                        \n",
      "                if bimodal_counter >= 3:\n",
      "                    for key in column_ids.keys():\n",
      "                        column_ids[key] = 'bimodal'       \n",
      "                \n",
      "                for position in range(len(temp_samples)):\n",
      "                    concatname = temp_samples[position]\n",
      "                    part1 = temp_samples[position][:-7]\n",
      "                    part2 = temp_samples[position][-5:]\n",
      "                    if column_ids[part2][:12] == 'undetermined':\n",
      "                        tempvalue = training_params[concatname] \n",
      "                        del training_params[concatname]\n",
      "                        machine_params[concatname] = tempvalue\n",
      "                        temp_samples[position] = 'XXXXXXXXXXXX' #not very elegant, I know...\n",
      "                        temp_categories[position] = 'XXXXXXXXXXXX'\n",
      "                        temp_parameters[position] = 'XXXXXXXXXXXX'\n",
      "                    if column_ids[part2][:7] == 'bimodal':\n",
      "                        temp_categories[position] = 'bimodal'\n",
      "                \n",
      "                temp_samples = [x for x in temp_samples if x != 'XXXXXXXXXXXX']\n",
      "                temp_categories = [x for x in temp_categories if x != 'XXXXXXXXXXXX']\n",
      "                temp_parameters = [x for x in temp_parameters if x != 'XXXXXXXXXXXX']\n",
      "                \n",
      "                for position in range(len(temp_categories)): #brute force removal of a bug that was resulting in strange category names\n",
      "                    if temp_categories[position][:6] == 'precip':\n",
      "                        temp_categories[position] = \"precipitation\"\n",
      "                \n",
      "                samples += temp_samples\n",
      "                categories += temp_categories\n",
      "                parameters += temp_parameters\n",
      "\n",
      "                results[current_filename] = column_ids\n",
      "\n",
      "        if process_all_pickles == True:\n",
      "            suffix = str(pickle_number)+'.json'\n",
      "            with open('result_dict'+suffix, 'w+') as f:\n",
      "                f.write(json.dumps(results))\n",
      "\n",
      "    if process_all_pickles == True:\n",
      "        with open('categories.json', 'w+') as f:\n",
      "            f.write(json.dumps(categories))\n",
      "        with open('samples.json', 'w+') as f:\n",
      "            f.write(json.dumps(samples))\n",
      "        with open('parameters.json', 'w+') as f:\n",
      "            f.write(json.dumps(parameters))\n",
      "        with open('reject_params.json', 'w+') as f:\n",
      "            f.write(json.dumps(reject_params))\n",
      "        with open('machine_params.json', 'w+') as f:\n",
      "            f.write(json.dumps(machine_params))\n",
      "        with open('training_params.json', 'w+') as f:\n",
      "            f.write(json.dumps(training_params))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# auxiliary script to print median per month, then quantiles for a single column\n",
      "# will only work if in above block, process_all_pickles == False and do_part != -1\n",
      "# (if do_part = -1, it will work, but will show the last filename in the block)\n",
      "# (if process_all_pickles == False, it will show the last filename in the entire dataset)\n",
      "\n",
      "print pickle_number, current_filename\n",
      "cc = 'raw10'\n",
      "for i in range(1, 13):\n",
      "    print cc, i,\n",
      "    print dfsub[(dfsub[cc] > -49) & (dfsub.month == i)][cc].median()\n",
      "\n",
      "for i in range(1, 13):\n",
      "    print cc, i,\n",
      "    print dfsub[(dfsub[cc] > -49) & (dfsub.dayness == i)][cc].median()    \n",
      "\n",
      "for i in range(2, 100, 2):\n",
      "    print str(i).rjust(3),\n",
      "    print str(dfsub[cc].quantile(i/100.0)).ljust(8)\n",
      "\n",
      "x = dfsub[(dfsub[cc] > -49)][cc]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9 WAS0708.DAT\n",
        "raw10 1 16.315\n",
        "raw10 2 25.09\n",
        "raw10 3 23.505\n",
        "raw10 4 31.75\n",
        "raw10 5 81.2\n",
        "raw10 6 69.535\n",
        "raw10 7 59.23\n",
        "raw10 8 51.34\n",
        "raw10 9 45.445\n",
        "raw10 10 32.66\n",
        "raw10 11 60.7\n",
        "raw10 12 54.775\n",
        "raw10 1 40.365\n",
        "raw10 2 40.025\n",
        "raw10 3 39.82\n",
        "raw10 4 39.75\n",
        "raw10 5 40.15\n",
        "raw10 6 41.465\n",
        "raw10 7 43.49\n",
        "raw10 8 46.095\n",
        "raw10 9 48.805\n",
        "raw10 10 50.415\n",
        "raw10 11 52.91\n",
        "raw10 12 53.84\n",
        "  2 11.0128 \n",
        "  4 13.0428 \n",
        "  6 14.48   \n",
        "  8 15.83   \n",
        " 10 17.18   \n",
        " 12 18.722  \n",
        " 14 20.0    \n",
        " 16 21.2112 \n",
        " 18 22.28   \n",
        " 20 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "23.46   \n",
        " 22 24.6304 \n",
        " 24 25.8068 \n",
        " 26 26.75   \n",
        " 28 27.99   \n",
        " 30 29.192  \n",
        " 32 30.4924 \n",
        " 34 31.6076 \n",
        " 36 32.42   \n",
        " 38 33.56   \n",
        " 40 34.998  \n",
        " 42 36.4744 \n",
        " 44 38.05   \n",
        " 46 39.89   \n",
        " 48 41.66   \n",
        " 50 43.46   \n",
        " 52 45.5164 \n",
        " 54 47.2584 \n",
        " 56 49.19   \n",
        " 58 51.04   \n",
        " 60 52.98   \n",
        " 62 54.7    \n",
        " 64 56.164  \n",
        " 66 57.8236 \n",
        " 68 59.22   \n",
        " 70 60.504  \n",
        " 72 61.57   \n",
        " 74 62.7468 \n",
        " 76 64.09   \n",
        " 78 65.44   \n",
        " 80 67.07   \n",
        " 82 68.9    \n",
        " 84 70.6    \n",
        " 86 72.7    \n",
        " 88 75.0    \n",
        " 90 77.3    \n",
        " 92 79.544  \n",
        " 94 82.4    \n",
        " 96 86.7    \n",
        " 98 92.9    \n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "### 5. PERFORM MACHINE LEARNING (RANDOM FOREST METHOD) TO CLASSIFY UNDETERMINED COLUMNS\n",
      "###\n",
      "\n",
      "do_this_part = True\n",
      "\n",
      "if do_this_part == True:\n",
      "\n",
      "    categories = json.loads(open('categories.json', 'r').read())\n",
      "    samples = json.loads(open('samples.json', 'r').read())\n",
      "    parameters = json.loads(open('parameters.json', 'r').read())\n",
      "\n",
      "    # replace NaN with -999\n",
      "    for i in range(len(parameters)):\n",
      "        for j in range(len(parameters[i][0])):  # the [0] is to hack a bug that enclosed lists 3 deep instead of the intended 2\n",
      "            if np.isnan(parameters[i][0][j]):\n",
      "                parameters[i][0][j] = -999\n",
      "\n",
      "    # run 90% of data as training set, `0% as evaluation\n",
      "\n",
      "    subset_index = range(len(parameters))\n",
      "    shuffle(subset_index)\n",
      "    training_set_length = int(0.9 * len(parameters))\n",
      "\n",
      "    evaluation_index = subset_index[training_set_length:]\n",
      "    train_index = subset_index[:training_set_length]            \n",
      "\n",
      "    train_samples = []\n",
      "    train_categories = []\n",
      "    train_parameters = []\n",
      "\n",
      "    for i in range(len(train_index)):\n",
      "        train_samples.append(samples[subset_index[i]])\n",
      "        train_categories.append(categories[subset_index[i]])\n",
      "        train_parameters.append(parameters[subset_index[i]][0])\n",
      "\n",
      "    evaluation_samples = []\n",
      "    evaluation_categories = []\n",
      "    evaluation_parameters = []\n",
      "\n",
      "    for i in range(len(evaluation_index)):\n",
      "        evaluation_samples.append(samples[subset_index[i]])\n",
      "        evaluation_categories.append(categories[subset_index[i]])\n",
      "        evaluation_parameters.append(parameters[subset_index[i]][0])\n",
      "\n",
      "    forest = RandomForestClassifier(n_estimators=1000)\n",
      "    forest.fit(train_parameters, train_categories)\n",
      "\n",
      "    category_array = np.asarray(train_categories)\n",
      "\n",
      "    scores = cross_val_score(forest, train_parameters, category_array, cv=20)\n",
      "    print(\"Accuracy: %0.2f (+/- %0.2f)\"\n",
      "          % (scores.mean(), scores.std()*2))\n",
      "\n",
      "    print '\\nFeature importances:'\n",
      "    assert len(parameter_names) == len(forest.feature_importances_)\n",
      "    for i in range(len(parameter_names)):\n",
      "        print parameter_names[i].ljust(18), round(forest.feature_importances_[i], 3)\n",
      "\n",
      "    evaluations = []\n",
      "    for parameter_list in evaluation_parameters:\n",
      "        evaluations.append(forest.predict(parameter_list))\n",
      "\n",
      "    assert len(evaluation_categories) == len(evaluations)\n",
      "\n",
      "    eval_right = 0\n",
      "    for i in range(len(evaluations)):\n",
      "        if str(evaluation_categories[i]) == str(evaluations[i][0]):\n",
      "            eval_right += 1\n",
      "\n",
      "    print \"\\n\"\n",
      "    print \"Evaluation set: %d out of %d correct (%d percent)\" % (eval_right, len(evaluations), \n",
      "                                           int(eval_right * 100.0 / len(evaluations)))\n",
      "\n",
      "    pickle.dump( forest, open( \"forest.pickle\", \"wb+\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.91 (+/- 0.03)\n",
        "\n",
        "Feature importances:\n",
        "min                "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.054\n",
        "median             0.141\n",
        "max                "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.09\n",
        "interquartile      0.078\n",
        "stdev              "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.087\n",
        "float_prop         0.034\n",
        "winter_corr        "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.041\n",
        "day_corr           0.026\n",
        "jan_median         "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.19\n",
        "jul_median         0.153\n",
        "zero_prop          "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.074\n",
        "noon_midn_med_dif  0.032\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Evaluation set: 378 out of 381 correct (99 percent)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories = json.loads(open('categories.json', 'r').read())\n",
      "x = pd.Series(categories)\n",
      "x.unique()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "array([u'snow_total', u'precipitation', u'wind_dir', u'wind_max',\n",
        "       u'wind_avg', u'wind_min', u'humidity', u'snow_24', u'temperature',\n",
        "       u'battery_voltage', u'bimodal'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "### 6. Build results with predicted \"undetermined\" data\n",
      "###\n",
      "\n",
      "# concatenate results files into one\n",
      "results_all = {}\n",
      "for i in range(10):\n",
      "   results_all = dict(results_all.items() + json.loads(open('result_dict'+str(i)+'.json', 'r').read()).items())\n",
      "with open('results_all.json', 'w+') as f:\n",
      "    f.write(json.dumps(results_all))\n",
      "    \n",
      "training_params = json.loads(open('training_params.json', 'r').read())\n",
      "reject_params = json.loads(open('reject_params.json', 'r').read())\n",
      "machine_params = json.loads(open('machine_params.json', 'r').read())\n",
      "\n",
      "all_params = dict(training_params.items() + reject_params.items() + machine_params.items())\n",
      "\n",
      "# replace NaN with -999\n",
      "for key in training_params.keys():\n",
      "    for j in range(len(training_params[key][0])): #for some reason, it's a list inside a list\n",
      "        if np.isnan(training_params[key][0][j]):\n",
      "            training_params[key][0][j] = -999\n",
      "for key in reject_params.keys():\n",
      "    for j in range(len(reject_params[key][0])):\n",
      "        if np.isnan(reject_params[key][0][j]):\n",
      "            reject_params[key][0][j] = -999\n",
      "for key in machine_params.keys():\n",
      "    for j in range(len(machine_params[key][0])):\n",
      "        if np.isnan(machine_params[key][0][j]):\n",
      "            machine_params[key][0][j] = -999\n",
      "\n",
      "# make pandas dataframe to hold results\n",
      "dffinal = pd.DataFrame()\n",
      "for filename in results_all.keys():\n",
      "    for rawname in results_all[filename].keys():\n",
      "        dffinal = dffinal.append(pd.DataFrame({'filename':[filename], 'raw_col':[rawname], \n",
      "                                               'original_cat':[str(results_all[filename][rawname])]}),\n",
      "                                                ignore_index=True)\n",
      "\n",
      "dffinal.reset_index(drop=True, inplace=True)\n",
      "\n",
      "# populate new results dicts from old, with forest prediction\n",
      "\n",
      "labels = ['battery_voltage', 'bimodal', 'humidity', 'precipitation', 'snow_24', \n",
      "          'snow_total', 'temperature', 'wind_avg', 'wind_dir', 'wind_max', 'wind_min']\n",
      "\n",
      "forest = pickle.load( open( \"forest.pickle\", \"rb\" ) )\n",
      "\n",
      "# do machine learning prediction and build dict without numeric or \"_duplicate\" suffixes\n",
      "\n",
      "dffinal['initial_col'] = ''\n",
      "dffinal['final_col'] = ''\n",
      "dffinal['machine_prob'] = ''\n",
      "\n",
      "for pn in parameter_names:\n",
      "    dffinal[pn] = ''\n",
      "\n",
      "for idx, row in dffinal.iterrows():\n",
      "    concatname = dffinal.filename[idx] + '__' + dffinal.raw_col[idx]\n",
      "    origcol = dffinal.original_cat[idx]\n",
      "    if origcol[:12] == 'undetermined':\n",
      "        dffinal.initial_col[idx] = 'undetermined'\n",
      "        dffinal.final_col[idx] = str(list(forest.predict(machine_params[concatname]))[0])\n",
      "        dffinal.machine_prob[idx] = int(forest.predict_proba(machine_params[concatname]).max()*100)\n",
      "    elif origcol[:14] == 'undeterminable':\n",
      "        dffinal.initial_col[idx] = 'undeterminable'\n",
      "        dffinal.final_col[idx] = 'undeterminable'\n",
      "    else:\n",
      "        found = False\n",
      "        for label in labels:\n",
      "            if origcol[:len(label)] == label:\n",
      "                dffinal.initial_col[idx] = label\n",
      "                dffinal.final_col[idx] = label\n",
      "                found = True\n",
      "        if found == False:\n",
      "            raise Exception('Label not found')\n",
      "\n",
      "# number duplicate columns\n",
      "            \n",
      "dffinal.sort(['filename', 'final_col'], ascending=True, inplace=True)\n",
      "\n",
      "dffinal['final_numbered'] = dffinal.final_col\n",
      "\n",
      "filenames = dffinal.filename.unique()\n",
      "\n",
      "for filename in filenames:\n",
      "    counter_total = {}\n",
      "    counter = {}\n",
      "    for idx, row in dffinal[dffinal.filename == filename].iterrows():\n",
      "        increment_dict(counter_total, dffinal.final_col[idx])\n",
      "    for idx, row in dffinal[dffinal.filename == filename].iterrows():\n",
      "        colname = dffinal.final_col[idx]\n",
      "        if counter_total[colname] > 1 or colname == 'undeterminable':\n",
      "            increment_dict(counter, colname)\n",
      "            suffixnum = counter[colname]\n",
      "            suffixtxt = str(suffixnum)\n",
      "            if suffixnum < 10:\n",
      "                suffixtxt = '0' + suffixtxt\n",
      "            dffinal.final_numbered[idx] = colname + \"_\" + suffixtxt\n",
      "            \n",
      "# add parameters\n",
      "\n",
      "for idx, row in dffinal.iterrows():\n",
      "    concatname = dffinal.filename[idx] + '__' + dffinal.raw_col[idx]\n",
      "    for i in range(len(parameter_names)):\n",
      "        dffinal[parameter_names[i]][idx] = all_params[concatname][0][i]\n",
      "            # somehow it ended up being a list within a list, hence the [0]\n",
      "            \n",
      "dffinal.to_pickle('dffinal.pickle')\n",
      "dffinal.to_csv('categories_and_parameters.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "### 7. WRITE CSVs WITH REPLACED COLUMN NAMES\n",
      "###\n",
      "\n",
      "dffinal = pd.read_pickle('dffinal.pickle')\n",
      "\n",
      "col_list = dffinal.final_numbered.unique()\n",
      "\n",
      "for pickle_num in range(1,10):\n",
      "    pickle_name = 'wxdatamunged_' + str(pickle_num) + '.pickle'\n",
      "    out_prefix = 'wxdata_categorized_' + str(pickle_num)\n",
      "\n",
      "    start=time.time()\n",
      "\n",
      "    dfin = pd.read_pickle(pickle_name)\n",
      "    filenames = list(dfin.FileName.unique())\n",
      "\n",
      "    dfout = pd.DataFrame(columns=col_list)\n",
      "\n",
      "    for filename in filenames:\n",
      "        dftemp = dfin.copy()\n",
      "        dftemp = dftemp[dftemp.FileName == filename]\n",
      "        sub_list = []\n",
      "        for item in dftemp.columns:\n",
      "            if item[:3] == 'raw' and int(item[-2:]) >= 6:\n",
      "                sub_list.append(dffinal[(dffinal.filename == filename) & (dffinal.raw_col == item)].final_numbered.iloc[0])\n",
      "            else:\n",
      "                sub_list.append(item)\n",
      "        dftemp.columns = sub_list\n",
      "        dfout = pd.concat([dfout, dftemp])\n",
      "    \n",
      "    dfout.to_pickle(out_prefix+'.pickle')\n",
      "    dfout.to_csv(out_prefix+'.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
